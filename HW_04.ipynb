{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Homework 4**\n",
    "\n",
    "Due: **October 22, 5pm** (late submission until October 25th, 5pm -- no submission possible afterwards)\n",
    "\n",
    "Coding assignment: 25 points\n",
    "\n",
    "Project report: 15 points\n",
    "\n",
    "### Name: [TODO]\n",
    "\n",
    "### Link to the github repo: [TODO]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the evironment test below, make sure you get all green checks. If not, you will lose 2 points for each red or missing sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from packaging.version import parse as Version\n",
    "from platform import python_version\n",
    "\n",
    "OK = '\\x1b[42m[ OK ]\\x1b[0m'\n",
    "FAIL = \"\\x1b[41m[FAIL]\\x1b[0m\"\n",
    "\n",
    "try:\n",
    "    import importlib\n",
    "except ImportError:\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % sys.version)\n",
    "\n",
    "def import_version(pkg, min_ver, fail_msg=\"\"):\n",
    "    mod = None\n",
    "    try:\n",
    "        mod = importlib.import_module(pkg)\n",
    "        if pkg in {'PIL'}:\n",
    "            ver = mod.VERSION\n",
    "        else:\n",
    "            ver = mod.__version__\n",
    "        if Version(ver) == Version(min_ver):\n",
    "            print(OK, \"%s version %s is installed.\"\n",
    "                  % (lib, min_ver))\n",
    "        else:\n",
    "            print(FAIL, \"%s version %s is required, but %s installed.\"\n",
    "                  % (lib, min_ver, ver))    \n",
    "    except ImportError:\n",
    "        print(FAIL, '%s not installed. %s' % (pkg, fail_msg))\n",
    "    return mod\n",
    "\n",
    "\n",
    "# first check the python version\n",
    "pyversion = Version(python_version())\n",
    "\n",
    "if pyversion >= Version(\"3.12.11\"):\n",
    "    print(OK, \"Python version is %s\" % pyversion)\n",
    "elif pyversion < Version(\"3.12.11\"):\n",
    "    print(FAIL, \"Python version 3.12.11 is required,\"\n",
    "                \" but %s is installed.\" % pyversion)\n",
    "else:\n",
    "    print(FAIL, \"Unknown Python version: %s\" % pyversion)\n",
    "\n",
    "    \n",
    "print()\n",
    "requirements = {'matplotlib': \"3.10.5\", 'numpy': \"2.3.2\",'sklearn': \"1.7.1\", \n",
    "                'pandas': \"2.3.2\", 'pytest': \"8.4.1\", 'torch':\"2.7.1\"}\n",
    "\n",
    "# now the dependencies\n",
    "for lib, required_version in list(requirements.items()):\n",
    "    import_version(lib, required_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Coding Assignment** (25 points)\n",
    "\n",
    "### Introduction\n",
    "In this assignment, you'll implement Binary Logistic Regression with\n",
    "regularization to perform classification. This classification task is to\n",
    "predict whether or not a given patient has breast cancer based on health\n",
    "data. The regularization method that you will be using is Tikhonov\n",
    "regularization (L2 norm). You will also do cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stencil Code & Data\n",
    "\n",
    "We have provided the following stencil code within this file:\n",
    "\n",
    "-   `models` contains the `RegularizedLogisticRegression` model which\n",
    "    you will be implementing.\n",
    "    \n",
    "-   `main` is the entry point of your program which will read in the\n",
    "    data, run the classifier and print the results.\n",
    "\n",
    "-   `Check Model` contains a series of tests to ensure you are coding your \n",
    "    model properly.\n",
    "    \n",
    "You should not modify any code in the `main`. If you do for debugging\n",
    "or other purposes, please make sure any additions are commented out in\n",
    "the final handin. Do not modify or move the `Check Model` cell! If you \n",
    "do so, you will lose points. The unit tests in that cell make it easy \n",
    "to grade your solution. All the functions you need to fill in reside \n",
    "in this notebook, marked by `TODO`s. You can see a full description \n",
    "of them in the section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCI Breast Cancer Wisconsin (Diagnostic) Data Set \n",
    "\n",
    "You will use a preprocessed version of the Breast Cancer Wisconsin\n",
    "(Diagnostic) Data Set from UC Irvine's Machine Learning Repository site.\n",
    "You can read more about the dataset here at\n",
    "<https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)>.\n",
    "We have split it up into train and validation sets already\n",
    "for you and read them in in `main`.\n",
    "\n",
    "## **The Assignment**\n",
    "\n",
    "We provide you with a sigmoid function to use when training your data.\n",
    "In `models`, there are five functions you will implement. They are:\n",
    "\n",
    "-   `RegularizedLogisticRegression`:\n",
    "\n",
    "    -   **`train()`** uses batch stochastic gradient descent to learn\n",
    "        the weights. You may find your solution from HW03 to be helpful,\n",
    "        but in this assignment, we will train for a finite number of\n",
    "        epochs rather than until we reach a particular convergence\n",
    "        criteria. The weight update step for this assignment will also\n",
    "        be different from HW03.\n",
    "\n",
    "    -   **`predict()`** predicts the labels using the inputs of test\n",
    "        data.\n",
    "\n",
    "    -   **`accuracy()`** computes the percentage of the correctly\n",
    "        predicted labels over a dataset.\n",
    "\n",
    "    -   **`runTrainTestValSplit()`** trains and evaluates for multiple\n",
    "        values of the hyperparameter lambda. This function evaluates\n",
    "        models by using train/validation sets, and\n",
    "        returns lists of training and validation errors with respect to\n",
    "        each value of lambda.\n",
    "\n",
    "    -   **`runKFold()`** evaluates models by implementing k-fold cross\n",
    "        validation, and returns a list of errors with respect to each\n",
    "        value of lambda. Note that we have defined\n",
    "        `_kFoldSplitIndices()` for you, which you may find helpful when\n",
    "        implementing this function.\n",
    "\n",
    "*Note*: You are not allowed to use any off-the-shelf packages that have\n",
    "already implemented these models, such as scikit-learn. We're asking you\n",
    "to implement them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Binary Logistic Regression**\n",
    "\n",
    "Similar to homework 3, we are again implementing Logistic Regression for\n",
    "classification. However, note that there are a few key differences. For\n",
    "this assignment, we are performing binary classification, which is a\n",
    "special case of multi-class classification. We are also implementing\n",
    "regularization, so you should think about how you would need to modify\n",
    "the loss function and gradient provided below to include regularization.\n",
    "For this problem, there are only two classes, which are denoted by\n",
    "$\\{0, 1\\}$ labels.\\\n",
    "Our model will perform the following:\n",
    "\n",
    "$$h(x) = \\frac{1}{1 + e^{-<w, x>}}$$\n",
    "\n",
    "where $w$ is the model's weights and $h(x)$ is the probability that the\n",
    "data point $x$ has a label of 1. We have implemented this as\n",
    "`sigmoid_function()` for you.\\\n",
    "\\\n",
    "Our loss function will be Binary Log Loss, also called Binary Cross\n",
    "Entropy Loss:\n",
    "\n",
    "$$L_S(h) = -\\frac{1}{m} \\sum_{i=1}^m (y_i \\log h(x_i) + (1 - y_i)\\log (1 - h(x_i)))$$\n",
    "\n",
    "on a sample $S$ of $m$ data points. Therefore, the corresponding\n",
    "gradient of the Binary Log loss with respect to the model's weights is\n",
    "$$\\frac{\\partial L_S(h)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^m (h(x_i) - y_i)x_{ij}$$\n",
    "\n",
    "### **Regularize with Tikhonov Regularization**\n",
    "\n",
    "As mentioned in the introduction part, with Tikhonov regularization, you\n",
    "just need to implement the L2 norm of the weights, which is\n",
    "$$\\lambda||w||_2^2 = \\lambda\\sum_{i=1}^{d}w_i^2$$ \n",
    "\n",
    "With that added, the gradient used to update the weights has to be adjusted to include\n",
    "$$\\frac{\\partial \\lambda\\sum_{i=1}^{d}w_i^2}{\\partial w_j} = 2\\lambda w_j$$\n",
    "Notice that the $\\lambda$ parameter above is used to control the\n",
    "contribution of the regularization term to the overall learning process\n",
    "that you may have to tune a little bit when implementing the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid_function(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "class RegularizedLogisticRegression(object):\n",
    "    '''\n",
    "    Implement regularized logistic regression for binary classification.\n",
    "    The weight vector w should be learned by minimizing the regularized loss\n",
    "    L(h, (x,y)) = log(1 + exp(-y <w, x>)) + lambda |w|_2^2. In other words, the objective\n",
    "    function that we are trying to minimize is the log loss for binary logistic regression \n",
    "    plus Tikhonov regularization with a coefficient of lambda.\n",
    "    '''\n",
    "    def __init__(self, batch_size = 15):\n",
    "        self.learningRate = 0.00001 # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.num_epochs = 10000 # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.batch_size = batch_size # Feel free to play around with this if you'd like, though this value will do\n",
    "        self.weights = None\n",
    "        self.lmbda = 1 # tune this parameter\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        '''\n",
    "        Train the model, using batch stochastic gradient descent\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        #[TODO]\n",
    "        pass\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Compute predictions based on the learned parameters and examples X\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "        @return:\n",
    "            A 1D Numpy array with one element for each row in X containing the predicted class.\n",
    "        '''\n",
    "        #[TODO]\n",
    "        pass\n",
    "        \n",
    "\n",
    "    def accuracy(self,X, Y):\n",
    "        '''\n",
    "        Output the accuracy of the trained model on a given testing dataset X and labels Y.\n",
    "        @params:\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "        @return:\n",
    "            a float number indicating accuracy (between 0 and 1)\n",
    "        '''\n",
    "        #[TODO]\n",
    "        pass\n",
    "\n",
    "    def runTrainTestValSplit(self, lambda_list, X_train, Y_train, X_val, Y_val):\n",
    "        '''\n",
    "        Given the training and validation data, fit the model with training data and test it with\n",
    "        respect to each lambda. Record the training error and validation error, which are equivalent \n",
    "        to (1 - accuracy).\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X_train: a 2D Numpy array for trainig where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_train: a 1D Numpy array for training containing the corresponding labels for each example\n",
    "            X_val: a 2D Numpy array for validation where each row contains an example,\n",
    "            padded by 1 column for the bias\n",
    "            Y_val: a 1D Numpy array for validation containing the corresponding labels for each example\n",
    "        @returns:\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "        '''\n",
    "        train_errors = []\n",
    "        val_errors = []\n",
    "        #[TODO] train model and calculate train and validation errors here for each lambda\n",
    "\n",
    "        return train_errors, val_errors\n",
    "\n",
    "    def _kFoldSplitIndices(self, dataset, k):\n",
    "        '''\n",
    "        Helper function for k-fold cross validation. Evenly split the indices of a\n",
    "        dataset into k groups.\n",
    "        For example, indices = [0, 1, 2, 3] with k = 2 may have an output\n",
    "        indices_split = [[1, 3], [2, 0]].\n",
    "        \n",
    "        Please don't change this.\n",
    "        @params:\n",
    "            dataset: a Numpy array where each row contains an example\n",
    "            k: an integer, which is the number of folds\n",
    "        @return:\n",
    "            indices_split: a list containing k groups of indices\n",
    "        '''\n",
    "        num_data = dataset.shape[0]\n",
    "        fold_size = int(num_data / k)\n",
    "        indices = np.arange(num_data)\n",
    "        np.random.shuffle(indices)\n",
    "        indices_split = np.split(indices[:fold_size*k], k)\n",
    "        return indices_split\n",
    "\n",
    "    def runKFold(self, lambda_list, X, Y, k = 3):\n",
    "        '''\n",
    "        Run k-fold cross validation on X and Y with respect to each lambda. Return all k-fold\n",
    "        errors.\n",
    "        \n",
    "        Each run of k-fold involves k iterations. For an arbitrary iteration i, the i-th fold is\n",
    "        used as testing data while the rest k-1 folds are combined as one set of training data. The k results are\n",
    "        averaged as the cross validation error.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            X: a 2D Numpy array where each row contains an example, padded by 1 column for the bias\n",
    "            Y: a 1D Numpy array containing the corresponding labels for each example\n",
    "            k: an integer, which is the number of folds, k is 3 by default\n",
    "        @return:\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        '''\n",
    "        k_fold_errors = []\n",
    "        for lmbda in lambda_list:\n",
    "            self.lmbda = lmbda\n",
    "            #[TODO] call _kFoldSplitIndices to split indices into k groups randomly\n",
    "\n",
    "            #[TODO] for each iteration i = 1...k, train the model using lmbda\n",
    "            # on kâˆ’1 folds of data. Then test with the i-th fold.\n",
    "\n",
    "            #[TODO] calculate and record the cross validation error by averaging total errors\n",
    "\n",
    "        return k_fold_errors\n",
    "\n",
    "    def plotError(self, lambda_list, train_errors, val_errors, k_fold_errors):\n",
    "        '''\n",
    "        Produce a plot of the cost function on the training and validation sets, and the\n",
    "        cost function of k-fold with respect to the regularization parameter lambda. Use this plot\n",
    "        to determine a valid lambda.\n",
    "        @params:\n",
    "            lambda_list: a list of lambdas\n",
    "            train_errors: a list of training errors with respect to the lambda_list\n",
    "            val_errors: a list of validation errors with respect to the lambda_list\n",
    "            k_fold_errors: a list of k-fold errors with respect to the lambda_list\n",
    "        @return:\n",
    "            None\n",
    "        '''\n",
    "        plt.figure()\n",
    "        plt.semilogx(lambda_list, train_errors, label = 'training error')\n",
    "        plt.semilogx(lambda_list, val_errors, label = 'validation error')\n",
    "        plt.semilogx(lambda_list, k_fold_errors, label = 'k-fold error')\n",
    "        plt.xlabel('lambda')\n",
    "        plt.ylabel('error')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Check Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "# Sets random seed for testing purposes\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "# Creates Test Models\n",
    "test_model1 = RegularizedLogisticRegression(3)\n",
    "test_model2 = RegularizedLogisticRegression(3)\n",
    "\n",
    "# Creates Test Data\n",
    "x_bias = np.array([[0,4,1], [0,3,1], [5,0,1], [4,1,1], [0,5,1]])\n",
    "y = np.array([0,0,1,1,0])\n",
    "x_bias_test = np.array([[0,0,1], [-5,3,1], [9,0,1], [1,0,1], [6,-7,1]])\n",
    "y_test = np.array([0,0,1,0,1])\n",
    "\n",
    "x_bias2 = np.array([[0,0,1], [0,3,1], [4,0,1], [6,1,1], [0,1,1], [0,4,1]])\n",
    "y2 = np.array([0,1,1,1,0,1])\n",
    "x_bias_test2 = np.array([[0,0,1], [-5,-3,1], [9,0,1], [1,0,1]])\n",
    "y_test2 = np.array([0,0,1,0])\n",
    "\n",
    "\n",
    "# Test Train Model and Checks Model Weights\n",
    "test_model1.train(x_bias, y)\n",
    "weights1 = test_model1.weights\n",
    "assert isinstance(weights1, np.ndarray)\n",
    "assert weights1.ndim==2 and weights1.shape == (1,3)\n",
    "assert weights1 == pytest.approx(np.array([[0.12661045, -0.14658517, -0.01241918]]), 0.05)\n",
    "\n",
    "test_model2.train(x_bias2, y2)\n",
    "weights2 = test_model2.weights\n",
    "assert isinstance(weights2, np.ndarray)\n",
    "assert weights2.ndim==2 and weights2.shape == (1,3)\n",
    "assert weights2 == pytest.approx(np.array([[0.11113, 0.08361, 0.01943]]), 0.05) \n",
    "\n",
    "# Test Model Predict\n",
    "predict1 = test_model1.predict(x_bias_test)\n",
    "assert isinstance(predict1, np.ndarray)\n",
    "assert predict1.ndim==1 and predict1.shape==(5,)\n",
    "assert (predict1 == np.array([0, 0, 1, 1, 1])).all()\n",
    "\n",
    "predict2 = test_model2.predict(x_bias_test2)\n",
    "assert isinstance(predict2, np.ndarray)\n",
    "assert predict2.ndim==1 and predict2.shape==(4,)\n",
    "assert (test_model2.predict(x_bias_test2) == np.array([1, 0, 1, 1])).all()\n",
    "\n",
    "# Test Model Accuracy\n",
    "accuracy1 = test_model1.accuracy(x_bias_test, y_test)\n",
    "assert isinstance(accuracy1, float)\n",
    "assert accuracy1 == .8\n",
    "\n",
    "accuracy2 = test_model2.accuracy(x_bias_test2, y_test2)\n",
    "assert isinstance(accuracy2, float)\n",
    "assert accuracy2 == .5\n",
    "\n",
    "from datetime import date\n",
    "#[TODO] Print your name and the date, using today function from date "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract():\n",
    "    X_train = pd.read_csv('data/X_train.csv',header=None)\n",
    "    Y_train = pd.read_csv('data/y_train.csv',header=None)\n",
    "    X_val = pd.read_csv('data/X_val.csv',header=None)\n",
    "    Y_val = pd.read_csv('data/y_val.csv',header=None)\n",
    "\n",
    "    Y_train = np.array([i[0] for i in Y_train.values])\n",
    "    Y_val = np.array([i[0] for i in Y_val.values])\n",
    "\n",
    "    X_train = np.append(X_train, np.ones((len(X_train), 1)), axis=1)\n",
    "    X_val = np.append(X_val, np.ones((len(X_val), 1)), axis=1)\n",
    "\n",
    "    return X_train, X_val, Y_train, Y_val\n",
    "\n",
    "def main():\n",
    "    X_train, X_val, Y_train, Y_val = extract()\n",
    "    X_train_val = np.concatenate((X_train, X_val))\n",
    "    Y_train_val = np.concatenate((Y_train, Y_val))\n",
    "\n",
    "    RR = RegularizedLogisticRegression()\n",
    "    RR.train(X_train, Y_train)\n",
    "    print('Train Accuracy: ' + str(RR.accuracy(X_train, Y_train)))\n",
    "    print('Validation Accuracy: ' + str(RR.accuracy(X_val, Y_val)))\n",
    "\n",
    "    #[TODO] Once implemented, uncomment the following lines of code and:\n",
    "    # 1. implement runTrainTestValSplit to get the training and validation errors of our 70-15-15\n",
    "    #    split to the original dataset\n",
    "    # 2. implement runKFold to generate errors of each lambda, where k = 3 in this assignment\n",
    "    # 3. call plotError to plot those errors with respect to lambdas\n",
    "    '''\n",
    "    lambda_list = [1000, 100, 10, 1, 0.1, 0.01, 0.001]\n",
    "    train_errors, val_errors = RR.runTrainTestValSplit(lambda_list, X_train, Y_train, X_val, Y_val)\n",
    "    k_fold_errors = RR.runKFold(lambda_list, X_train_val, Y_train_val, 3)\n",
    "    print(lambda_list)\n",
    "    print(train_errors, val_errors, k_fold_errors)\n",
    "    RR.plotError(lambda_list, train_errors, val_errors, k_fold_errors)\n",
    "    '''\n",
    "    \n",
    "# Set random seeds. DO NOT CHANGE THIS IN YOUR FINAL SUBMISSION.\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Project Report** (15 points)\n",
    "\n",
    "### **Question 1**\n",
    "Briefly explain how you used batch stochastic gradient descent with\n",
    "regularization to learn the weights. Think about how the\n",
    "regularization is incorporated into the loss function and how that\n",
    "affects the gradient when updating weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 2**\n",
    "\n",
    "Use `plotError()`, which we have implemented for you, to produce a\n",
    "model selection curve. Include your plot here. Then, conclude what\n",
    "the best value of lambda is and explain why. <br>\n",
    "*Note: It takes about 10-15 minutes to generate a graph.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question 3**\n",
    "In this project, you used validation data to select a model. Suppose\n",
    "that each patient might've had multiple samples (e.g., multiple lab\n",
    "tests or x-rays) collected and entered into the dataset. Would you\n",
    "need to account for this when splitting your train-validation-test\n",
    "data? If yes, how? If no, why not? (3-5 sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "bcd9bc17ffadb8b3c09124f861805f4f094648af93180b87f0218364b7d0c0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
